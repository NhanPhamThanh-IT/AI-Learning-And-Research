{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <div align=\"center\" style=\"color: brown\"><strong>Logistic Regression Tutorial</strong></div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <div style=\"color: red\"><strong>Part 1. Introduction to Logistic Regression</strong></div>\n",
                "\n",
                "**Logistic Regression** is a fundamental classification algorithm in machine learning. Unlike linear regression, which predicts continuous values, logistic regression is used to predict the probability of a binary outcome (e.g., yes/no, 0/1, true/false).\n",
                "\n",
                "### Key Concepts\n",
                "- **Binary Classification**: Logistic regression is mainly used for problems where the target variable has two possible outcomes.\n",
                "- **Sigmoid Function**: The core of logistic regression is the sigmoid (logistic) function, which maps any real-valued number into the (0, 1) interval, representing probability.\n",
                "- **Decision Boundary**: The model predicts class 1 if the probability is above a threshold (commonly 0.5), otherwise class 0.\n",
                "\n",
                "### Mathematical Formulation\n",
                "The logistic regression model predicts the probability $p$ that the target variable $y$ is 1 given input features $X$:\n",
                "\n",
                "$p = \\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
                "where\n",
                "$z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n$\n",
                "\n",
                "- $\\sigma(z)$ is the sigmoid function.\n",
                "- $\\beta_0$ is the intercept (bias).\n",
                "- $\\beta_1, ..., \\beta_n$ are the coefficients (weights).\n",
                "\n",
                "### Loss Function\n",
                "Logistic regression uses the **log-loss** (binary cross-entropy) as its cost function:\n",
                "\n",
                "$\\text{Loss} = -\\frac{1}{m} \\sum_{i=1}^m [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)]$\n",
                "\n",
                "### Assumptions\n",
                "- The outcome is binary.\n",
                "- Observations are independent.\n",
                "- There is little or no multicollinearity among the independent variables.\n",
                "- The relationship between the independent variables and the log-odds is linear.\n",
                "\n",
                "### Applications\n",
                "- Medical diagnosis (disease vs. no disease)\n",
                "- Email spam detection\n",
                "- Credit scoring (default vs. no default)\n",
                "- Marketing (buy vs. not buy)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <div style=\"color: red\"><strong>Part 2. Implementation in Python</strong></div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import make_classification\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
                ")\n",
                "\n",
                "# 1. Generate a synthetic binary classification dataset\n",
                "X, y = make_classification(\n",
                "    n_samples=200, n_features=2, n_redundant=0, n_clusters_per_class=1,\n",
                "    flip_y=0.1, class_sep=1.5, random_state=42\n",
                ")\n",
                "\n",
                "# 2. Split the dataset into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.25, random_state=42\n",
                ")\n",
                "\n",
                "# 3. Create and train the logistic regression model\n",
                "model = LogisticRegression()\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "# 4. Make predictions\n",
                "y_pred = model.predict(X_test)\n",
                "y_proba = model.predict_proba(X_test)[:, 1]\n",
                "\n",
                "# 5. Evaluate the model\n",
                "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
                "print(\"Confusion Matrix:\", confusion_matrix(y_test, y_pred))\n",
                "print(\"Classification Report:\", classification_report(y_test, y_pred))\n",
                "\n",
                "# 6. Plot the decision boundary\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='bwr', edgecolor='k', alpha=0.7, label='Test data')\n",
                "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
                "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
                "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
                "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "Z = Z.reshape(xx.shape)\n",
                "plt.contourf(xx, yy, Z, alpha=0.2, cmap='bwr')\n",
                "plt.xlabel('Feature 1')\n",
                "plt.ylabel('Feature 2')\n",
                "plt.title('Logistic Regression Decision Boundary')\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "# 7. Plot ROC curve\n",
                "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
                "roc_auc = auc(fpr, tpr)\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
                "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
                "plt.legend(loc=\"lower right\")\n",
                "plt.grid(True)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <div style=\"color: red\"><strong>Part 3. Interpretation and Best Practices</strong></div>\n",
                "\n",
                "- **Interpretation of Coefficients**: The coefficients represent the change in the log-odds of the outcome for a one-unit increase in the predictor.\n",
                "- **Feature Scaling**: Not strictly required, but can help with convergence.\n",
                "- **Regularization**: Use L1 (Lasso) or L2 (Ridge) regularization to prevent overfitting, especially with many features.\n",
                "- **Multiclass Extension**: Logistic regression can be extended to multiclass problems using the \"one-vs-rest\" or \"softmax\" (multinomial) approach.\n",
                "\n",
                "### Advantages\n",
                "- Simple and fast to train\n",
                "- Outputs well-calibrated probabilities\n",
                "- Interpretable coefficients\n",
                "\n",
                "### Limitations\n",
                "- Assumes linear relationship between features and log-odds\n",
                "- Not suitable for non-linear problems without feature engineering\n",
                "- Sensitive to outliers\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
