# Nesterov Accelerated Gradient

This directory contains a tutorial on the Nesterov Accelerated Gradient (NAG) algorithm. NAG is a optimization technique that is a refinement of the Momentum method, often resulting in faster convergence.

It includes:
- Explanation of the NAG algorithm and how it differs from standard Momentum.
- Implementation of Nesterov Accelerated Gradient.
- Comparison with Momentum and standard Gradient Descent. 