{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\" style=\"color: brown\"><strong>Multiple Linear Regression Tutorial</strong></div>\n",
    "\n",
    "## <div style=\"color: red\"><strong>Part 1. Introduction to Multiple Linear Regression</strong></div>\n",
    "\n",
    "Multiple linear regression is an extension of simple linear regression that allows us to model the relationship between a dependent variable and multiple independent variables. While simple linear regression has one independent variable (X) and one dependent variable (Y), multiple linear regression has two or more independent variables.\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "The mathematical equation for multiple linear regression is:\n",
    "\n",
    "$$Y = β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ + ε$$\n",
    "\n",
    "Where:\n",
    "- Y is the dependent variable (what we're trying to predict)\n",
    "- X₁, X₂, ..., Xₙ are the independent variables (features)\n",
    "- β₀ is the y-intercept (constant term)\n",
    "- β₁, β₂, ..., βₙ are the coefficients for each independent variable\n",
    "- ε is the error term (residual)\n",
    "\n",
    "### Key Differences from Simple Linear Regression\n",
    "\n",
    "- **Multiple predictors**: Uses two or more independent variables to predict the outcome\n",
    "- **Higher dimensional space**: While simple linear regression models a line, multiple regression models a plane (2 predictors) or hyperplane (3+ predictors)\n",
    "- **More complex interpretation**: Each coefficient represents the change in Y for a one-unit change in Xᵢ while holding all other variables constant\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "Multiple linear regression relies on the same assumptions as simple linear regression:\n",
    "\n",
    "1. **Linearity**: The relationship between independent and dependent variables is linear\n",
    "2. **Independence**: Observations are independent of each other\n",
    "3. **Homoscedasticity**: Constant variance in errors\n",
    "4. **Normality**: Residuals are normally distributed\n",
    "5. **No multicollinearity**: Independent variables are not highly correlated with each other (unique to multiple regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 2. Implementing Multiple Linear Regression in Python</strong></div>\n",
    "\n",
    "Let's start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set the style for our plots\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating a Synthetic Dataset\n",
    "\n",
    "Let's create a synthetic dataset to demonstrate multiple linear regression. We'll simulate a housing price dataset with three features: house size (in square feet), number of bedrooms, and age of the house (in years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create 100 samples\n",
    "n_samples = 100\n",
    "\n",
    "# Generate features\n",
    "house_size = np.random.normal(1500, 500, n_samples)  # House size in square feet\n",
    "bedrooms = np.random.randint(1, 6, n_samples)        # Number of bedrooms (1-5)\n",
    "house_age = np.random.normal(15, 10, n_samples)      # House age in years\n",
    "\n",
    "# Generate target variable (house price) with some noise\n",
    "# Formula: price = 50000 + 100*size + 25000*bedrooms - 5000*age + noise\n",
    "price = 50000 + 100 * house_size + 25000 * bedrooms - 5000 * house_age + np.random.normal(0, 25000, n_samples)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Size': house_size,\n",
    "    'Bedrooms': bedrooms,\n",
    "    'Age': house_age,\n",
    "    'Price': price\n",
    "})\n",
    "\n",
    "# Display the first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Exploratory Data Analysis\n",
    "\n",
    "Let's explore our dataset to better understand the relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix and visualize it\n",
    "correlation_matrix = data.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairplots to visualize relationships between all variables\n",
    "sns.pairplot(data, height=2.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual scatter plots with regression lines for each feature vs price\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Size vs Price\n",
    "sns.regplot(x='Size', y='Price', data=data, ax=axes[0])\n",
    "axes[0].set_title('Size vs Price', fontsize=16)\n",
    "axes[0].set_xlabel('House Size (sq ft)', fontsize=14)\n",
    "axes[0].set_ylabel('Price ($)', fontsize=14)\n",
    "\n",
    "# Bedrooms vs Price\n",
    "sns.regplot(x='Bedrooms', y='Price', data=data, ax=axes[1])\n",
    "axes[1].set_title('Bedrooms vs Price', fontsize=16)\n",
    "axes[1].set_xlabel('Number of Bedrooms', fontsize=14)\n",
    "axes[1].set_ylabel('Price ($)', fontsize=14)\n",
    "\n",
    "# Age vs Price\n",
    "sns.regplot(x='Age', y='Price', data=data, ax=axes[2])\n",
    "axes[2].set_title('Age vs Price', fontsize=16)\n",
    "axes[2].set_xlabel('House Age (years)', fontsize=14)\n",
    "axes[2].set_ylabel('Price ($)', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Preparing the Data for Modeling\n",
    "\n",
    "Let's split our data into training and testing sets and prepare it for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target variable (y)\n",
    "X = data[['Size', 'Bedrooms', 'Age']]\n",
    "y = data['Price']\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features (optional but often beneficial)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} samples\")\n",
    "print(f\"Testing set size: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 3. Training and Evaluating the Multiple Linear Regression Model</strong></div>\n",
    "\n",
    "Now let's train our model using both regular and scaled features to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with original (unscaled) features\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Train another model with scaled features\n",
    "model_scaled = LinearRegression()\n",
    "model_scaled.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model coefficients and intercept for unscaled model\n",
    "print(\"Unscaled Model:\")\n",
    "print(f\"Intercept: ${model.intercept_:.2f}\")\n",
    "print(\"Coefficients:\")\n",
    "for feature, coef in zip(X.columns, model.coef_):\n",
    "    print(f\"- {feature}: ${coef:.2f} per unit change\")\n",
    "\n",
    "print(\"\\nInterpreting these coefficients:\")\n",
    "print(f\"- For each additional square foot, the house price increases by ${model.coef_[0]:.2f}\")\n",
    "print(f\"- For each additional bedroom, the house price increases by ${model.coef_[1]:.2f}\")\n",
    "print(f\"- For each additional year of age, the house price decreases by ${-model.coef_[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "mse_scaled = mean_squared_error(y_test, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "r2_scaled = r2_score(y_test, y_pred_scaled)\n",
    "\n",
    "print(\"Unscaled Model Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): ${mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): ${rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "print(\"\\nScaled Model Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): ${mse_scaled:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): ${rmse_scaled:.2f}\")\n",
    "print(f\"R² Score: {r2_scaled:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Actual vs Predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Price', fontsize=14)\n",
    "plt.ylabel('Predicted Price', fontsize=14)\n",
    "plt.title('Actual vs Predicted House Prices', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals to check for patterns\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.7)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Price', fontsize=14)\n",
    "plt.ylabel('Residuals', fontsize=14)\n",
    "plt.title('Residual Plot', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot a histogram of residuals to check normality\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.xlabel('Residual Value', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title('Distribution of Residuals', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 4. Feature Importance and Model Interpretation</strong></div>\n",
    "\n",
    "Let's analyze which features have the most significant impact on our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standardized coefficients (for better comparison)\n",
    "coef_scaled = model_scaled.coef_\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Raw Coefficient': model.coef_,\n",
    "    'Standardized Coefficient': coef_scaled\n",
    "})\n",
    "\n",
    "feature_importance = feature_importance.sort_values(by='Standardized Coefficient', key=abs, ascending=False)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Standardized Coefficient'])\n",
    "plt.xlabel('Standardized Coefficient', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "plt.title('Feature Importance', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 5. Making Predictions with the Model</strong></div>\n",
    "\n",
    "Let's use our model to make predictions for new houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some new house data\n",
    "new_houses = pd.DataFrame({\n",
    "    'Size': [1200, 1800, 2500, 3000],\n",
    "    'Bedrooms': [2, 3, 4, 5],\n",
    "    'Age': [20, 10, 5, 1]\n",
    "})\n",
    "\n",
    "# Make predictions\n",
    "new_predictions = model.predict(new_houses)\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "new_houses['Predicted Price'] = new_predictions\n",
    "\n",
    "# Display the results\n",
    "new_houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 6. Advanced Techniques for Multiple Regression</strong></div>\n",
    "\n",
    "### 6.1 Adding Polynomial Features\n",
    "\n",
    "Sometimes, the relationship between features and the target is not strictly linear. We can add polynomial terms to our model to capture non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create polynomial features (degree=2 includes squared terms and interaction terms)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = poly.get_feature_names_out(X.columns)\n",
    "print(f\"Original features: {X.columns.tolist()}\")\n",
    "print(f\"Polynomial features: {feature_names.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model with polynomial features\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Make predictions with the polynomial model\n",
    "y_pred_poly = poly_model.predict(X_test_poly)\n",
    "\n",
    "# Calculate performance metrics\n",
    "mse_poly = mean_squared_error(y_test, y_pred_poly)\n",
    "rmse_poly = np.sqrt(mse_poly)\n",
    "r2_poly = r2_score(y_test, y_pred_poly)\n",
    "\n",
    "print(\"Polynomial Model Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): ${mse_poly:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): ${rmse_poly:.2f}\")\n",
    "print(f\"R² Score: {r2_poly:.4f}\")\n",
    "print(\"\\nCompare with original model:\")\n",
    "print(f\"Original model R²: {r2:.4f}\")\n",
    "print(f\"Improvement: {(r2_poly-r2)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Dealing with Multicollinearity\n",
    "\n",
    "Multicollinearity occurs when two or more independent variables are highly correlated with each other. This can cause issues with coefficient interpretation. Let's demonstrate how to detect and handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with multicollinearity\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Create two highly correlated features\n",
    "base_feature = np.random.normal(0, 1, n_samples)\n",
    "feature1 = base_feature + np.random.normal(0, 0.1, n_samples)  # Feature 1 is very similar to base_feature\n",
    "feature2 = base_feature + np.random.normal(0, 0.1, n_samples)  # Feature 2 is very similar to base_feature\n",
    "feature3 = np.random.normal(0, 1, n_samples)  # Independent feature\n",
    "\n",
    "# Generate target variable\n",
    "target = 3 * base_feature + 2 * feature3 + np.random.normal(0, 1, n_samples)\n",
    "\n",
    "# Create a DataFrame\n",
    "collinear_data = pd.DataFrame({\n",
    "    'Feature1': feature1,\n",
    "    'Feature2': feature2,\n",
    "    'Feature3': feature3,\n",
    "    'Target': target\n",
    "})\n",
    "\n",
    "# Check the correlation matrix\n",
    "corr_matrix = collinear_data.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix - Multicollinearity Example', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance Inflation Factor (VIF) is a common way to detect multicollinearity. VIF measures how much the variance of an estimated regression coefficient increases if your predictors are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Calculate VIF\n",
    "X_collinear = collinear_data[['Feature1', 'Feature2', 'Feature3']]\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_collinear.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_collinear.values, i) for i in range(X_collinear.shape[1])]\n",
    "\n",
    "print(\"VIF Values:\")\n",
    "print(vif_data)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- VIF = 1: No multicollinearity\")\n",
    "print(\"- 1 < VIF < 5: Moderate multicollinearity\")\n",
    "print(\"- 5 < VIF < 10: High multicollinearity\")\n",
    "print(\"- VIF > 10: Severe multicollinearity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Regularization: Ridge and Lasso Regression\n",
    "\n",
    "To handle multicollinearity and overfitting, we can use regularization techniques like Ridge and Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# Prepare training and testing data\n",
    "X_collinear_train, X_collinear_test, y_collinear_train, y_collinear_test = train_test_split(\n",
    "    X_collinear, collinear_data['Target'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_collinear_train_scaled = scaler.fit_transform(X_collinear_train)\n",
    "X_collinear_test_scaled = scaler.transform(X_collinear_test)\n",
    "\n",
    "# Train OLS, Ridge, and Lasso models\n",
    "ols_model = LinearRegression().fit(X_collinear_train_scaled, y_collinear_train)\n",
    "ridge_model = Ridge(alpha=1.0).fit(X_collinear_train_scaled, y_collinear_train)\n",
    "lasso_model = Lasso(alpha=0.1).fit(X_collinear_train_scaled, y_collinear_train)\n",
    "\n",
    "# Compare coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X_collinear.columns,\n",
    "    'OLS': ols_model.coef_,\n",
    "    'Ridge': ridge_model.coef_,\n",
    "    'Lasso': lasso_model.coef_\n",
    "})\n",
    "\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficients\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.25\n",
    "x = np.arange(len(coef_df['Feature']))\n",
    "\n",
    "plt.bar(x - bar_width, coef_df['OLS'], width=bar_width, label='OLS', color='blue')\n",
    "plt.bar(x, coef_df['Ridge'], width=bar_width, label='Ridge', color='green')\n",
    "plt.bar(x + bar_width, coef_df['Lasso'], width=bar_width, label='Lasso', color='red')\n",
    "\n",
    "plt.xlabel('Feature', fontsize=14)\n",
    "plt.ylabel('Coefficient Value', fontsize=14)\n",
    "plt.title('Comparison of Regression Coefficients', fontsize=16)\n",
    "plt.xticks(x, coef_df['Feature'])\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "models = {'OLS': ols_model, 'Ridge': ridge_model, 'Lasso': lasso_model}\n",
    "model_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_collinear_test_scaled)\n",
    "    r2 = r2_score(y_collinear_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_collinear_test, y_pred))\n",
    "    model_scores[name] = {'R²': r2, 'RMSE': rmse}\n",
    "    \n",
    "pd.DataFrame(model_scores).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 7. Summary and Conclusion</strong></div>\n",
    "\n",
    "In this tutorial, we've covered:\n",
    "\n",
    "1. **Theory of Multiple Linear Regression**\n",
    "   - Mathematical representation and key concepts\n",
    "   - Differences from simple linear regression\n",
    "   - Assumptions of the model\n",
    "\n",
    "2. **Implementing Multiple Linear Regression in Python**\n",
    "   - Creating and exploring synthetic datasets\n",
    "   - Preparing data for modeling\n",
    "   - Training and evaluating the model\n",
    "   - Feature importance and interpretation\n",
    "\n",
    "3. **Advanced Techniques**\n",
    "   - Polynomial regression for non-linear relationships\n",
    "   - Detecting and handling multicollinearity\n",
    "   - Regularization with Ridge and Lasso regression\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Multiple linear regression extends simple linear regression by incorporating multiple predictors\n",
    "- Feature scaling can be important for fair comparison of coefficients\n",
    "- Polynomial features can help capture non-linear relationships\n",
    "- Multicollinearity can be detected using correlation matrices and VIF\n",
    "- Ridge and Lasso regression can help manage multicollinearity and prevent overfitting\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Apply multiple regression to real-world datasets\n",
    "- Explore techniques for feature selection\n",
    "- Learn about other regression techniques like Random Forest or Gradient Boosting\n",
    "- Study model validation techniques like cross-validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
