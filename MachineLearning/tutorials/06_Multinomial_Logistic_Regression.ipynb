{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\" style=\"color: brown\"><strong>Logistic Regression: A Comprehensive Tutorial</strong></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d6ae9",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 1. Theoretical Background</strong></div>\n",
    "\n",
    "### What is Logistic Regression?\n",
    "Logistic Regression is a supervised machine learning algorithm used for classification tasks. Unlike linear regression, which predicts continuous values, logistic regression predicts the probability of a categorical outcome.\n",
    "\n",
    "- **Binary Logistic Regression**: Used when the target variable has two classes (e.g., spam/not spam).\n",
    "- **Multiclass Logistic Regression**: Used when the target variable has more than two classes (e.g., digit recognition).\n",
    "\n",
    "### Mathematical Formulation\n",
    "For binary classification, the model predicts the probability $p$ that $y=1$ given input $X$:\n",
    "$p = \\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "where $z = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n$\n",
    "\n",
    "For multiclass, logistic regression can use either the one-vs-rest (OvR) or multinomial (softmax) approach.\n",
    "\n",
    "### Loss Function\n",
    "- **Binary cross-entropy (log-loss)** for binary classification\n",
    "- **Multinomial cross-entropy** for multiclass\n",
    "\n",
    "### Regularization\n",
    "- **L1 (Lasso)**: Can shrink some coefficients to zero (feature selection)\n",
    "- **L2 (Ridge)**: Shrinks coefficients but does not set them to zero\n",
    "- Regularization helps prevent overfitting\n",
    "\n",
    "### Feature Scaling\n",
    "- Not strictly required, but helps with convergence and regularization\n",
    "- StandardScaler or MinMaxScaler are commonly used\n",
    "\n",
    "### Assumptions\n",
    "- The outcome is categorical\n",
    "- Observations are independent\n",
    "- Little or no multicollinearity among predictors\n",
    "- Linear relationship between features and log-odds\n",
    "\n",
    "### Applications\n",
    "- Medical diagnosis\n",
    "- Email spam detection\n",
    "- Credit scoring\n",
    "- Image and text classification\n",
    "\n",
    "### Limitations\n",
    "- Assumes linearity in log-odds\n",
    "- Not suitable for highly non-linear problems without feature engineering\n",
    "- Sensitive to outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1742d5c1",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 2. Binary Logistic Regression: Implementation and Evaluation</strong></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51034e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# 1. Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=300, n_features=3, n_redundant=0, n_clusters_per_class=1,\n",
    "    flip_y=0.1, class_sep=1.5, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Train logistic regression with L2 regularization\n",
    "model = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 5. Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# 6. Evaluation\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:', classification_report(y_test, y_pred))\n",
    "\n",
    "# 7. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 8. Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "ap = average_precision_score(y_test, y_proba)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(recall, precision, label=f'AP = {ap:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4134658",
   "metadata": {},
   "source": [
    "### Visualizing the Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced3cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b14a8",
   "metadata": {},
   "source": [
    "### Interpreting Coefficients\n",
    "\n",
    "The coefficients in logistic regression represent the change in the log-odds of the outcome for a one-unit increase in the predictor (after scaling).\n",
    "\n",
    "- Positive coefficient: increases the log-odds (probability) of the positive class\n",
    "- Negative coefficient: decreases the log-odds\n",
    "\n",
    "Let's display the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b766f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, coef in enumerate(model.coef_[0]):\n",
    "    print(f'Feature {i+1}: {coef:.3f}')\n",
    "print('Intercept:', model.intercept_[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9315b4",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 3. Multiclass Logistic Regression</strong></div>\n",
    "\n",
    "Logistic regression can be extended to multiclass problems using two main approaches:\n",
    "- **One-vs-Rest (OvR)**: Fits one classifier per class\n",
    "- **Multinomial (Softmax)**: Fits a single model with a softmax function\n",
    "\n",
    "Let's demonstrate both on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96315492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# One-vs-Rest (OvR)\n",
    "ovr_model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
    "ovr_model.fit(X_scaled, y)\n",
    "y_pred_ovr = ovr_model.predict(X_scaled)\n",
    "print('One-vs-Rest Classification Report:', classification_report(y, y_pred_ovr))\n",
    "print('Confusion Matrix (OvR):', confusion_matrix(y, y_pred_ovr))\n",
    "\n",
    "# Multinomial (Softmax)\n",
    "multi_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
    "multi_model.fit(X_scaled, y)\n",
    "y_pred_multi = multi_model.predict(X_scaled)\n",
    "print('Multinomial Classification Report:', classification_report(y, y_pred_multi))\n",
    "print('Confusion Matrix (Multinomial):', confusion_matrix(y, y_pred_multi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 4. Regularization and Hyperparameter Tuning</strong></div>\n",
    "\n",
    "Regularization helps prevent overfitting. The strength of regularization is controlled by the parameter $C$ (inverse of regularization strength).\n",
    "\n",
    "Let's use GridSearchCV to find the best $C$ for our binary example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']  # liblinear supports both l1 and l2\n",
    "}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print('Best parameters:', grid.best_params_)\n",
    "print('Best cross-validated accuracy:', grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 5. Best Practices, Tips, and Summary</strong></div>\n",
    "\n",
    "- Always scale features when using regularization.\n",
    "- Use cross-validation to tune hyperparameters (C, penalty).\n",
    "- For imbalanced data, consider using class_weight='balanced' or resampling.\n",
    "- Check for multicollinearity among features.\n",
    "- Interpret coefficients in the context of scaled features.\n",
    "- Use ROC and precision-recall curves for model evaluation, especially with imbalanced data.\n",
    "- Logistic regression is a strong baseline for many classification problems.\n",
    "\n",
    "### Further Reading\n",
    "- [scikit-learn LogisticRegression documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- [Logistic Regression (Wikipedia)](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "- [Interpreting Logistic Regression Coefficients](https://www.theanalysisfactor.com/interpret-coefficients-logistic-regression/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
