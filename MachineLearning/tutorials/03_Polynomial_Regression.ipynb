{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c82ec3a",
   "metadata": {},
   "source": [
    "# <div align=\"center\" style=\"color: brown\"><strong>Polynomial Regression Tutorial</strong></div>\n",
    "\n",
    "## <div style=\"color: red\"><strong>Part 1. Introduction to Polynomial Regression</strong></div>\n",
    "\n",
    "Polynomial regression is an extension of linear regression that allows us to model non-linear relationships between variables. While linear regression fits a straight line to data, polynomial regression fits a curve by including polynomial terms (squared, cubed, etc.) of the independent variable.\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "The mathematical equation for polynomial regression is:\n",
    "\n",
    "$$Y = β₀ + β₁X + β₂X^2 + β₃X^3 + ... + βₙX^n + ε$$\n",
    "\n",
    "Where:\n",
    "- Y is the dependent variable (what we're trying to predict)\n",
    "- X is the independent variable\n",
    "- X^2, X^3, ..., X^n are the polynomial terms of X\n",
    "- β₀ is the y-intercept (constant term)\n",
    "- β₁, β₂, ..., βₙ are the coefficients for each term\n",
    "- ε is the error term (residual)\n",
    "- n is the degree of the polynomial\n",
    "\n",
    "### Key Differences from Linear Regression\n",
    "\n",
    "- **Non-linear relationships**: Can model curved relationships between variables\n",
    "- **Flexibility**: Higher degree polynomials can capture more complex patterns in data\n",
    "- **Overfitting risk**: Higher degree polynomials may fit training data too closely, leading to poor generalization\n",
    "\n",
    "### When to Use Polynomial Regression\n",
    "\n",
    "1. When there's a clear non-linear pattern in your data\n",
    "2. When a simple linear model doesn't adequately capture the relationship\n",
    "3. When you need to model curved relationships, peaks, or valleys in your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794db2fe",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 2. Implementing Polynomial Regression in Python</strong></div>\n",
    "\n",
    "Let's start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Set the style for our plots\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07134958",
   "metadata": {},
   "source": [
    "### 2.1 Creating a Synthetic Dataset\n",
    "\n",
    "Let's create a synthetic dataset that demonstrates a non-linear relationship between variables. We'll create data that follows a cubic polynomial pattern with some added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d5868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 100 points between -10 and 10\n",
    "n_samples = 100\n",
    "X = np.linspace(-10, 10, n_samples).reshape(-1, 1)\n",
    "\n",
    "# Generate target variable following a cubic polynomial: y = 1 + 2x + 3x² - 0.5x³ + noise\n",
    "true_coef = [1, 2, 3, -0.5]  # Coefficients: constant, x, x², x³\n",
    "y_true = true_coef[0] + true_coef[1]*X.flatten() + true_coef[2]*X.flatten()**2 + true_coef[3]*X.flatten()**3\n",
    "y = y_true + np.random.normal(0, 20, n_samples)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'X': X.flatten(),\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "# Display the first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5719fc3e",
   "metadata": {},
   "source": [
    "### 2.2 Visualizing the Dataset\n",
    "\n",
    "Let's visualize our dataset to see the non-linear pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8e8584",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['X'], data['y'], alpha=0.7)\n",
    "plt.xlabel('X', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.title('Non-linear Relationship Between X and y', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb0deed",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 3. Comparing Linear and Polynomial Models</strong></div>\n",
    "\n",
    "Let's first fit a simple linear regression model to see how it performs on our non-linear data. Then we'll compare it with polynomial regression models of different degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0326eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit a linear regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Create polynomial regression models with different degrees\n",
    "models = {\n",
    "    'Linear': make_pipeline(LinearRegression()),\n",
    "    'Quadratic': make_pipeline(PolynomialFeatures(degree=2, include_bias=False), LinearRegression()),\n",
    "    'Cubic': make_pipeline(PolynomialFeatures(degree=3, include_bias=False), LinearRegression()),\n",
    "    'Quartic': make_pipeline(PolynomialFeatures(degree=4, include_bias=False), LinearRegression()),\n",
    "    'Degree 5': make_pipeline(PolynomialFeatures(degree=5, include_bias=False), LinearRegression()),\n",
    "    'Degree 10': make_pipeline(PolynomialFeatures(degree=10, include_bias=False), LinearRegression())\n",
    "}\n",
    "\n",
    "# Fit all models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "predictions = {}\n",
    "scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    predictions[name] = model.predict(X_test)\n",
    "    scores[name] = {\n",
    "        'R² Score': r2_score(y_test, predictions[name]),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, predictions[name]))\n",
    "    }\n",
    "    \n",
    "# Display performance metrics\n",
    "performance = pd.DataFrame(scores).T\n",
    "print(\"Model Performance on Test Data:\")\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdacb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the fitted models\n",
    "# Create a smooth grid of points for plotting\n",
    "X_grid = np.linspace(X.min(), X.max(), 1000).reshape(-1, 1)\n",
    "\n",
    "# Plot data and models\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X, y, color='blue', alpha=0.5, label='Data Points')\n",
    "\n",
    "# Plot the true curve\n",
    "y_true_grid = true_coef[0] + true_coef[1]*X_grid.flatten() + true_coef[2]*X_grid.flatten()**2 + true_coef[3]*X_grid.flatten()**3\n",
    "plt.plot(X_grid, y_true_grid, 'k-', linewidth=2.5, label='True Relationship')\n",
    "\n",
    "# Plot the predictions for each model\n",
    "colors = ['red', 'green', 'purple', 'orange', 'brown', 'magenta']\n",
    "i = 0\n",
    "for name, model in models.items():\n",
    "    y_grid_pred = model.predict(X_grid)\n",
    "    plt.plot(X_grid, y_grid_pred, color=colors[i], linewidth=2, label=f'{name} Model')\n",
    "    i += 1\n",
    "\n",
    "plt.title('Comparison of Polynomial Regression Models', fontsize=16)\n",
    "plt.xlabel('X', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a9081",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 4. Understanding Polynomial Regression Coefficients</strong></div>\n",
    "\n",
    "In polynomial regression, we have coefficients for each power of X. Let's extract and interpret these coefficients from our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6c5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients from the cubic model (degree 3)\n",
    "cubic_model = models['Cubic']\n",
    "poly_features = cubic_model.named_steps['polynomialfeatures']\n",
    "linear_reg = cubic_model.named_steps['linearregression']\n",
    "\n",
    "# Get feature names for the polynomial terms\n",
    "feature_names = poly_features.get_feature_names_out(['X'])\n",
    "\n",
    "# Create DataFrame with coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Term': feature_names,\n",
    "    'Coefficient': linear_reg.coef_\n",
    "})\n",
    "\n",
    "# Add intercept\n",
    "coef_df = pd.concat([pd.DataFrame({'Term': ['Intercept'], 'Coefficient': [linear_reg.intercept_]}), coef_df])\n",
    "\n",
    "print(\"Cubic Model Coefficients:\")\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863628a7",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 5. Residual Analysis</strong></div>\n",
    "\n",
    "Residual analysis helps us evaluate model performance and check if our assumptions are met. Let's analyze the residuals for our linear and cubic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c01c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals for linear and cubic models\n",
    "linear_residuals = y_test - predictions['Linear']\n",
    "cubic_residuals = y_test - predictions['Cubic']\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot residuals vs. predicted values for linear model\n",
    "axes[0, 0].scatter(predictions['Linear'], linear_residuals)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 0].set_title('Linear Model: Residuals vs Predicted', fontsize=14)\n",
    "axes[0, 0].set_xlabel('Predicted values', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Residuals', fontsize=12)\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Plot residuals vs. predicted values for cubic model\n",
    "axes[0, 1].scatter(predictions['Cubic'], cubic_residuals)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 1].set_title('Cubic Model: Residuals vs Predicted', fontsize=14)\n",
    "axes[0, 1].set_xlabel('Predicted values', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Histogram of residuals for linear model\n",
    "axes[1, 0].hist(linear_residuals, bins=15, edgecolor='black')\n",
    "axes[1, 0].set_title('Linear Model: Residuals Distribution', fontsize=14)\n",
    "axes[1, 0].set_xlabel('Residual Value', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Histogram of residuals for cubic model\n",
    "axes[1, 1].hist(cubic_residuals, bins=15, edgecolor='black')\n",
    "axes[1, 1].set_title('Cubic Model: Residuals Distribution', fontsize=14)\n",
    "axes[1, 1].set_xlabel('Residual Value', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17e0007",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 6. Understanding Overfitting in Polynomial Regression</strong></div>\n",
    "\n",
    "While higher-degree polynomials can better fit training data, they risk overfitting, which means they perform poorly on new data. Let's demonstrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d43e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller dataset to emphasize overfitting\n",
    "np.random.seed(42)\n",
    "n_samples_small = 15\n",
    "X_small = np.linspace(-10, 10, n_samples_small).reshape(-1, 1)\n",
    "y_true_small = true_coef[0] + true_coef[1]*X_small.flatten() + \\\n",
    "               true_coef[2]*X_small.flatten()**2 + true_coef[3]*X_small.flatten()**3\n",
    "y_small = y_true_small + np.random.normal(0, 20, n_samples_small)\n",
    "\n",
    "# Create models with different degrees\n",
    "degrees = [1, 3, 10, 20]\n",
    "models_small = {}\n",
    "\n",
    "for degree in degrees:\n",
    "    model_name = f'Degree {degree}'\n",
    "    models_small[model_name] = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression())\n",
    "    models_small[model_name].fit(X_small, y_small)\n",
    "\n",
    "# Create a smooth grid for plotting\n",
    "X_grid = np.linspace(X_small.min(), X_small.max(), 1000).reshape(-1, 1)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the true curve\n",
    "y_true_grid = true_coef[0] + true_coef[1]*X_grid.flatten() + \\\n",
    "              true_coef[2]*X_grid.flatten()**2 + true_coef[3]*X_grid.flatten()**3\n",
    "plt.plot(X_grid, y_true_grid, 'k-', linewidth=2.5, label='True Relationship')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X_small, y_small, color='blue', s=60, alpha=0.7, label='Data Points')\n",
    "\n",
    "# Plot each model\n",
    "colors = ['red', 'green', 'purple', 'orange']\n",
    "for i, (name, model) in enumerate(models_small.items()):\n",
    "    y_pred = model.predict(X_grid)\n",
    "    plt.plot(X_grid, y_pred, color=colors[i], linewidth=2, label=f'{name}')\n",
    "\n",
    "plt.title('Overfitting in Polynomial Regression', fontsize=16)\n",
    "plt.xlabel('X', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfedc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training and test errors for different polynomial degrees\n",
    "degrees = list(range(1, 21))  # Degrees from 1 to 20\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Generate a larger dataset for more reliable testing\n",
    "np.random.seed(42)\n",
    "n_samples_large = 500\n",
    "X_large = np.linspace(-10, 10, n_samples_large).reshape(-1, 1)\n",
    "y_true_large = true_coef[0] + true_coef[1]*X_large.flatten() + \\\n",
    "               true_coef[2]*X_large.flatten()**2 + true_coef[3]*X_large.flatten()**3\n",
    "y_large = y_true_large + np.random.normal(0, 20, n_samples_large)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(\n",
    "    X_large, y_large, test_size=0.3, random_state=42)\n",
    "\n",
    "# Calculate errors for each degree\n",
    "for degree in degrees:\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train_large)\n",
    "    X_test_poly = poly.transform(X_test_large)\n",
    "    \n",
    "    # Fit model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train_large)\n",
    "    \n",
    "    # Calculate errors\n",
    "    train_pred = model.predict(X_train_poly)\n",
    "    test_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train_large, train_pred)\n",
    "    test_mse = mean_squared_error(y_test_large, test_pred)\n",
    "    \n",
    "    train_errors.append(np.sqrt(train_mse))  # RMSE\n",
    "    test_errors.append(np.sqrt(test_mse))    # RMSE\n",
    "\n",
    "# Plot the errors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, train_errors, marker='o', linestyle='-', color='blue', label='Training RMSE')\n",
    "plt.plot(degrees, test_errors, marker='o', linestyle='-', color='red', label='Test RMSE')\n",
    "\n",
    "# Add vertical line at degree 3 (our true model)\n",
    "plt.axvline(x=3, color='green', linestyle='--', label='True Model Degree (3)')\n",
    "\n",
    "plt.title('Training vs. Test Error for Different Polynomial Degrees', fontsize=16)\n",
    "plt.xlabel('Polynomial Degree', fontsize=14)\n",
    "plt.ylabel('Root Mean Squared Error (RMSE)', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d4b9b0",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 7. Regularization to Prevent Overfitting</strong></div>\n",
    "\n",
    "To prevent overfitting when using high-degree polynomials, we can apply regularization techniques like Ridge or Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a9283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define models with different regularization techniques\n",
    "degree = 10  # High degree polynomial\n",
    "models_reg = {\n",
    "    'No Regularization': make_pipeline(\n",
    "        PolynomialFeatures(degree=degree),\n",
    "        LinearRegression()\n",
    "    ),\n",
    "    'Ridge (alpha=1)': make_pipeline(\n",
    "        PolynomialFeatures(degree=degree),\n",
    "        Ridge(alpha=1)\n",
    "    ),\n",
    "    'Ridge (alpha=10)': make_pipeline(\n",
    "        PolynomialFeatures(degree=degree),\n",
    "        Ridge(alpha=10)\n",
    "    ),\n",
    "    'Lasso (alpha=0.1)': make_pipeline(\n",
    "        PolynomialFeatures(degree=degree),\n",
    "        Lasso(alpha=0.1, max_iter=10000)\n",
    "    ),\n",
    "    'Lasso (alpha=1)': make_pipeline(\n",
    "        PolynomialFeatures(degree=degree),\n",
    "        Lasso(alpha=1, max_iter=10000)\n",
    "    )\n",
    "}\n",
    "\n",
    "# Fit all models on the small dataset to emphasize overfitting\n",
    "for name, model in models_reg.items():\n",
    "    model.fit(X_small, y_small)\n",
    "\n",
    "# Create predictions on a grid\n",
    "y_preds_reg = {}\n",
    "for name, model in models_reg.items():\n",
    "    y_preds_reg[name] = model.predict(X_grid)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the true curve\n",
    "plt.plot(X_grid, y_true_grid, 'k-', linewidth=2.5, label='True Relationship')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X_small, y_small, color='blue', s=60, alpha=0.7, label='Data Points')\n",
    "\n",
    "# Plot each model\n",
    "colors = ['red', 'green', 'purple', 'orange', 'magenta']\n",
    "for i, (name, y_pred) in enumerate(y_preds_reg.items()):\n",
    "    plt.plot(X_grid, y_pred, color=colors[i], linewidth=2, label=f'{name}')\n",
    "\n",
    "plt.title(f'Regularized Polynomial Regression (Degree {degree})', fontsize=16)\n",
    "plt.xlabel('X', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b42d03",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 8. Real-World Application Example</strong></div>\n",
    "\n",
    "Let's apply polynomial regression to a slightly more realistic scenario. We'll create a dataset that mimics the relationship between advertising spending and sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee902fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic advertising-sales data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create advertising spending data (in thousands of dollars)\n",
    "advertising = np.random.uniform(0, 50, 100).reshape(-1, 1)\n",
    "\n",
    "# Create sales data with diminishing returns (in thousands of dollars)\n",
    "# Formula: sales = 10 + 3*advertising - 0.03*advertising^2 + noise\n",
    "sales = 10 + 3 * advertising.flatten() - 0.03 * advertising.flatten()**2 + np.random.normal(0, 5, 100)\n",
    "\n",
    "# Create a DataFrame\n",
    "ad_data = pd.DataFrame({\n",
    "    'Advertising': advertising.flatten(),\n",
    "    'Sales': sales\n",
    "})\n",
    "\n",
    "# Show the first few rows\n",
    "ad_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e901780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(ad_data['Advertising'], ad_data['Sales'], alpha=0.7)\n",
    "plt.xlabel('Advertising Spend (thousands $)', fontsize=14)\n",
    "plt.ylabel('Sales (thousands $)', fontsize=14)\n",
    "plt.title('Relationship Between Advertising and Sales', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_ad = ad_data[['Advertising']].values\n",
    "y_ad = ad_data['Sales'].values\n",
    "X_ad_train, X_ad_test, y_ad_train, y_ad_test = train_test_split(X_ad, y_ad, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit models with different degrees\n",
    "ad_models = {\n",
    "    'Linear': make_pipeline(PolynomialFeatures(degree=1), LinearRegression()),\n",
    "    'Quadratic': make_pipeline(PolynomialFeatures(degree=2), LinearRegression()),\n",
    "    'Cubic': make_pipeline(PolynomialFeatures(degree=3), LinearRegression())\n",
    "}\n",
    "\n",
    "for name, model in ad_models.items():\n",
    "    model.fit(X_ad_train, y_ad_train)\n",
    "\n",
    "# Create a smooth grid for visualization\n",
    "X_ad_grid = np.linspace(X_ad.min(), X_ad.max(), 1000).reshape(-1, 1)\n",
    "\n",
    "# Make predictions\n",
    "ad_predictions = {}\n",
    "ad_scores = {}\n",
    "\n",
    "for name, model in ad_models.items():\n",
    "    y_pred = model.predict(X_ad_test)\n",
    "    ad_scores[name] = {\n",
    "        'R²': r2_score(y_ad_test, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_ad_test, y_pred))\n",
    "    }\n",
    "    ad_predictions[name] = model.predict(X_ad_grid)\n",
    "\n",
    "# Display metrics\n",
    "ad_performance = pd.DataFrame(ad_scores).T\n",
    "print(\"Model Performance on Test Data:\")\n",
    "ad_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X_ad, y_ad, color='blue', alpha=0.6, label='Data Points')\n",
    "\n",
    "# Plot the model predictions\n",
    "colors = ['red', 'green', 'purple']\n",
    "for i, (name, y_pred) in enumerate(ad_predictions.items()):\n",
    "    plt.plot(X_ad_grid, y_pred, color=colors[i], linewidth=2, label=f'{name} Model')\n",
    "\n",
    "plt.title('Advertising-Sales Relationship: Model Comparison', fontsize=16)\n",
    "plt.xlabel('Advertising Spend (thousands $)', fontsize=14)\n",
    "plt.ylabel('Sales (thousands $)', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the best model (Quadratic) to find the point of diminishing returns\n",
    "quadratic_model = ad_models['Quadratic']\n",
    "\n",
    "# Extract coefficients from the quadratic model\n",
    "poly = quadratic_model.named_steps['polynomialfeatures']\n",
    "linreg = quadratic_model.named_steps['linearregression']\n",
    "\n",
    "# Coefficients: β₀ + β₁x + β₂x²\n",
    "intercept = linreg.intercept_\n",
    "coef = linreg.coef_\n",
    "\n",
    "print(f\"Quadratic Model Equation: Sales = {intercept:.2f} + {coef[0]:.2f}×Advertising + {coef[1]:.2f}×Advertising²\")\n",
    "\n",
    "# Calculate derivative: dy/dx = β₁ + 2×β₂x\n",
    "# At maximum/minimum point, dy/dx = 0, so x = -β₁/(2×β₂)\n",
    "if coef[1] != 0:  # Ensure we don't divide by zero\n",
    "    optimal_advertising = -coef[0] / (2 * coef[1])\n",
    "    \n",
    "    if optimal_advertising > 0 and optimal_advertising < X_ad.max():\n",
    "        optimal_sales = intercept + coef[0] * optimal_advertising + coef[1] * optimal_advertising**2\n",
    "        print(f\"\\nOptimum Advertising Spending: ${optimal_advertising:.2f} thousand\")\n",
    "        print(f\"Estimated Maximum Sales: ${optimal_sales:.2f} thousand\")\n",
    "        \n",
    "        # If it's a maximum (negative coefficient for x²)\n",
    "        if coef[1] < 0:\n",
    "            print(\"After this point, diminishing returns will reduce the effectiveness of additional advertising spend.\")\n",
    "        else:\n",
    "            print(\"This is a minimum point. Sales increase at an increasing rate after this point.\")\n",
    "    else:\n",
    "        print(\"\\nOptimum point is outside the range of our data.\")\n",
    "else:\n",
    "    print(\"\\nThis is a linear relationship, not a curve with an optimum point.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99048da",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 9. Summary and Conclusion</strong></div>\n",
    "\n",
    "In this tutorial, we've covered:\n",
    "\n",
    "1. **Theory of Polynomial Regression**\n",
    "   - Mathematical representation and key concepts\n",
    "   - Differences from linear regression\n",
    "   - When to use polynomial regression\n",
    "\n",
    "2. **Implementing Polynomial Regression in Python**\n",
    "   - Creating and visualizing synthetic datasets\n",
    "   - Training models with different polynomial degrees\n",
    "   - Comparing model performance\n",
    "\n",
    "3. **Advanced Concepts**\n",
    "   - Understanding overfitting in polynomial regression\n",
    "   - Regularization techniques to prevent overfitting\n",
    "   - Analyzing a real-world application (advertising-sales relationship)\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Polynomial regression allows us to model curved relationships between variables\n",
    "- Higher degree polynomials provide more flexibility but risk overfitting\n",
    "- Regularization techniques like Ridge and Lasso help prevent overfitting\n",
    "- Testing multiple model degrees helps identify the best trade-off between bias and variance\n",
    "- Real-world applications often show non-linear patterns that polynomial regression can capture effectively\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Apply polynomial regression to real datasets\n",
    "2. Explore other non-linear regression techniques like splines and local regression\n",
    "3. Learn about methods for automatic model selection\n",
    "4. Study cross-validation techniques for more robust model evaluation\n",
    "5. Consider other advanced regression techniques like Support Vector Regression or Gradient Boosting Regression"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
