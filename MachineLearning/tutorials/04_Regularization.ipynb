{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\" style=\"color: brown\"><strong>Regularization in Regression</strong></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 1. Introduction to Regularization</strong></div>\n",
    "\n",
    "Regularization is a technique used in regression models to prevent overfitting by adding a penalty term to the loss function. Overfitting occurs when a model learns the noise in the training data, resulting in poor generalization to new data. Regularization discourages complex models by penalizing large coefficients.\n",
    "\n",
    "### Why Regularization?\n",
    "- **Reduces overfitting**: Helps the model generalize better to unseen data.\n",
    "- **Controls model complexity**: Penalizes large weights, leading to simpler models.\n",
    "- **Improves stability**: Especially useful when features are highly correlated (multicollinearity).\n",
    "\n",
    "### Types of Regularization\n",
    "1. **Ridge Regression (L2 Regularization)**: Adds the sum of squared coefficients as a penalty.\n",
    "2. **Lasso Regression (L1 Regularization)**: Adds the sum of absolute coefficients as a penalty.\n",
    "3. **Elastic Net**: Combines both L1 and L2 penalties.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For a linear regression model:\n",
    "$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n + \\epsilon$$\n",
    "\n",
    "The regularized loss functions are:\n",
    "- **Ridge (L2):**\n",
    "  $$\text{Loss} = \\sum_{i=1}^m (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^n \\beta_j^2$$\n",
    "- **Lasso (L1):**\n",
    "  $$\text{Loss} = \\sum_{i=1}^m (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^n |\\beta_j|$$\n",
    "\n",
    "Where $\\lambda$ (alpha in sklearn) is the regularization strength. Higher $\\lambda$ means more regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 2. Implementing Regularization in Python</strong></div>\n",
    "\n",
    "Let's start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating a Synthetic Dataset\n",
    "\n",
    "We'll create a dataset with multicollinearity (highly correlated features) to demonstrate the effect of regularization."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X1 = np.random.normal(0, 1, n_samples)\n",
    "X2 = X1 + np.random.normal(0, 0.1, n_samples)  # Highly correlated with X1\n",
    "X3 = np.random.normal(0, 1, n_samples)\n",
    "\n",
    "# True coefficients\n",
    "coefs = [5, 2, -3]\n",
    "y = 3 + coefs[0]*X1 + coefs[1]*X2 + coefs[2]*X3 + np.random.normal(0, 2, n_samples)\n",
    "\n",
    "data = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'y': y})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualizing the Dataset\n",
    "\n",
    "Let's check the correlation between features and visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(data.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Preparing the Data\n",
    "\n",
    "We'll split the data and scale the features (important for regularization)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X = data[['X1', 'X2', 'X3']]\n",
    "y = data['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f'Training set size: {len(X_train)} samples')\n",
    "print(f'Testing set size: {len(X_test)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 3. Comparing Linear, Ridge, and Lasso Regression</strong></div>\n",
    "\n",
    "Let's train and compare the three models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train models\n",
    "linear = LinearRegression()\n",
    "ridge = Ridge(alpha=1.0)\n",
    "lasso = Lasso(alpha=0.5)\n",
    "\n",
    "linear.fit(X_train_scaled, y_train)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_linear = linear.predict(X_test_scaled)\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "y_pred_lasso = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Linear', 'Ridge', 'Lasso'],\n",
    "    'R2 Score': [r2_score(y_test, y_pred_linear), r2_score(y_test, y_pred_ridge), r2_score(y_test, y_pred_lasso)],\n",
    "    'RMSE': [np.sqrt(mean_squared_error(y_test, y_pred_linear)),\n",
    "             np.sqrt(mean_squared_error(y_test, y_pred_ridge)),\n",
    "             np.sqrt(mean_squared_error(y_test, y_pred_lasso))]\n",
    "})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Comparing Coefficients\n",
    "\n",
    "Let's see how regularization affects the learned coefficients."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "coefs_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Linear': linear.coef_,\n",
    "    'Ridge': ridge.coef_,\n",
    "    'Lasso': lasso.coef_\n",
    "})\n",
    "coefs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualizing Coefficient Shrinkage\n",
    "\n",
    "Let's plot the coefficients for each model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "coefs_df.set_index('Feature').plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Comparison of Coefficients: Linear vs Ridge vs Lasso', fontsize=16)\n",
    "plt.ylabel('Coefficient Value', fontsize=14)\n",
    "plt.xlabel('Feature', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: red\"><strong>Part 4. Summary</strong></div>\n",
    "\n",
    "- **Ridge Regression** (L2) shrinks coefficients but does not set them to zero. Useful for multicollinearity.\n",
    "- **Lasso Regression** (L1) can shrink some coefficients to exactly zero, performing feature selection.\n",
    "- Regularization helps prevent overfitting and improves model generalization.\n",
    "\n",
    "**Tip:** Tune the regularization strength ($\\lambda$ or `alpha`) using cross-validation for best results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
